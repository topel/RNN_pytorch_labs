{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P20WO8mbw3Sw",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#TP-intro-aux-RNNs-en-PyTorch\" data-toc-modified-id=\"TP-intro-aux-RNNs-en-PyTorch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TP intro aux RNNs en PyTorch</a></span></li><li><span><a href=\"#Manipulation-basique-de-RNNCell\" data-toc-modified-id=\"Manipulation-basique-de-RNNCell-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Manipulation basique de RNNCell</a></span></li><li><span><a href=\"#Manipulation-basique-de-RNN\" data-toc-modified-id=\"Manipulation-basique-de-RNN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Manipulation basique de RNN</a></span></li><li><span><a href=\"#Données-séquentielles\" data-toc-modified-id=\"Données-séquentielles-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Données séquentielles</a></span></li><li><span><a href=\"#Limites-des-RNNs-&quot;vanilla&quot;\" data-toc-modified-id=\"Limites-des-RNNs-&quot;vanilla&quot;-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Limites des RNNs \"vanilla\"</a></span></li><li><span><a href=\"#RNNs-bidirectionnels\" data-toc-modified-id=\"RNNs-bidirectionnels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>RNNs bidirectionnels</a></span></li><li><span><a href=\"#Gérer-les-séquences-de-taille-variable\" data-toc-modified-id=\"Gérer-les-séquences-de-taille-variable-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Gérer les séquences de taille variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution-1-:-zero-padding\" data-toc-modified-id=\"Solution-1-:-zero-padding-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Solution 1 : zero-padding</a></span></li><li><span><a href=\"#Solution-2-:-&quot;empaqueter&quot;-les-séquences-avec-pack_sequence\" data-toc-modified-id=\"Solution-2-:-&quot;empaqueter&quot;-les-séquences-avec-pack_sequence-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Solution 2 : \"empaqueter\" les séquences avec pack_sequence</a></span></li><li><span><a href=\"#Solution-3-:-pack_padded_sequence\" data-toc-modified-id=\"Solution-3-:-pack_padded_sequence-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Solution 3 : pack_padded_sequence</a></span></li><li><span><a href=\"#Application-:-mini-batch-de-sequences-de-tailles-variables-dans-un-RNN\" data-toc-modified-id=\"Application-:-mini-batch-de-sequences-de-tailles-variables-dans-un-RNN-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Application : mini-batch de sequences de tailles variables dans un RNN</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgP23yEyw3S4"
   },
   "source": [
    "# TP intro aux RNNs en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T17:56:28.925765Z",
     "start_time": "2019-11-27T17:56:28.916033Z"
    },
    "colab_type": "text",
    "id": "E49De5D7w3S6"
   },
   "source": [
    "Lorsque l'on travaille avec des données séquentielles (séries temporelles, phrases, etc.), l'ordre des entrées est crucial pour la tâche à accomplir. Les réseaux neuronaux récurrents (RNN) traitent les données séquentielles en tenant compte de l'entrée courante et de ce qui a été appris des entrées précédentes. Dans ce notebook, nous apprendrons comment encoder des séries temporelles, comment créer et former des RNNs.\n",
    "\n",
    "<img src=\"https://github.com/topel/RNN_pytorch_labs/blob/master/tp0_intro_RNN/local/images/rnn.png?raw=1\" width=550>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yB7Ffm0Jw3S9"
   },
   "source": [
    "* **Objectif:** Traiter les données séquentielles en tenant compte de l'entrée courante et de ce qui a été appris des entrées précédentes.\n",
    "* **Avantages:** \n",
    "    * Rendre compte de l'ordre et des entrées précédentes.\n",
    "    * Génération conditionnée pour générer des séquences.\n",
    "* **Désavantages:**Désavantages \n",
    "    * La prédiction à chaque pas de temps dépend de la prédiction précédente, il est donc difficile de paralléliser les calculs avec un RNN.\n",
    "    * Le traitement de longues séquences peut entraîner des problèmes de mémoire et de calcul.\n",
    "    * L'interprétabilité est difficile, mais il y a quelques [techniques](https://arxiv.org/abs/1506.02078) qui utilisent les activations des RNN pour voir quelles parties des entrées sont traitées. \n",
    "* **Remarque:** \n",
    "    * L'amélioration de l'architecture pour rendre les RNNs plus rapides et interprétables est un domaine de recherche en cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGVR83Zdw3S-"
   },
   "source": [
    "<img src=\"https://github.com/topel/RNN_pytorch_labs/blob/master/tp0_intro_RNN/local/images/rnn2.png?raw=1\" width=650>\n",
    "\n",
    "Passe \"forward\" d'un RNN pour un pas de temps $X_t$ :\n",
    "\n",
    "$h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h)$\n",
    "\n",
    "$y_t = W_{hy}h_t + b_y $\n",
    "\n",
    "$ P(y) = softmax(y_t) = \\frac{e^y}{\\sum e^y} $\n",
    "\n",
    "*avec*:\n",
    "\n",
    "* $X_t$ = input au temps t, dans $\\mathbb{R}^{NXE}$, avec $N$ la batch size, $E$ la dimension des features (des embeddings si on traite des mots)\n",
    "* $W_{hh}$ = poids des neurones cachés, dans $\\mathbb{R}^{HXH}$, avec $H$ la dim du hidden\n",
    "* $h_{t-1}$ = état caché au temps précédent, dans $\\mathbb{R}^{NXH}$\n",
    "* $W_{xh}$ = poids sur l'entrée, dans $\\mathbb{R}^{EXH}$\n",
    "* $b_h$ = biais des neurones cachés, dans $\\mathbb{R}^{HX1}$\n",
    "* $W_{hy}$ = poids de la sortie, dans $\\mathbb{R}^{HXC}$, avec $C$ le nombre de classes\n",
    "* $b_y$ = biais des neurones de sortie, dans $\\mathbb{R}^{CX1}$\n",
    "\n",
    "On répète ces calculs pour tous les pas de temps en entrée ($X_{t+1}, X_{t+2}, ..., X_{N})$ pour obtenir une prédiction en sortie à chaque pas de temps.\n",
    "\n",
    "**Remarque**: Au premier pas de temps, l'état caché précédent $h_{t-1}$ peut être soit un vecteur de zéros (\"non-conditionné\"), soit initialisé avec certaines valeurs tirées au hasard ou bien fixées par une condition (\"conditionné\").   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMX-lWNgw3TB"
   },
   "source": [
    "# Manipulation basique de RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:19:35.198738Z",
     "start_time": "2020-03-09T10:19:34.570512Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QrLjfJk0w3TD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:19:36.449716Z",
     "start_time": "2020-03-09T10:19:36.444038Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1advnso-w3TI"
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_size = 10 # taille max par input (on utilisera du masking pour les séquences qui sont plus petites que cette valeur)\n",
    "x_lengths = [8, 5, 4, 10, 5] # taille de chaque séquence en input \n",
    "embedding_dim = 100\n",
    "rnn_hidden_dim = 256\n",
    "output_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:26:50.128912Z",
     "start_time": "2019-11-28T08:26:50.123067Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2XrBKe5nw3TM",
    "outputId": "f32d408a-8a7f-4545-8152-eca36cbe3d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# Initialisation des inputs synthétiques\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "x_lengths = torch.tensor(x_lengths)\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:26:50.447425Z",
     "start_time": "2019-11-28T08:26:50.441961Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "T0GjPDdWw3Tb",
    "outputId": "efd739ae-2e32-4bf9-def7-a58f4a4a6654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initialisation des hidden states (états cachés) à zéro\n",
    "hidden_t = torch.zeros((batch_size, rnn_hidden_dim))\n",
    "print (hidden_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:26:50.786289Z",
     "start_time": "2019-11-28T08:26:50.778818Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KOEm3my2w3Tj",
    "outputId": "745aedcc-daf4-4c9b-fb44-ab307dc79ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNCell(100, 256)\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de la cellule RNN\n",
    "rnn_cell = nn.RNNCell(embedding_dim, rnn_hidden_dim)\n",
    "print (rnn_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:54:02.271712Z",
     "start_time": "2019-11-28T08:54:02.260136Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fGjv3Wuow3Tx",
    "outputId": "9f645ac7-86a4-4e3f-befe-4fdb0bd9b71c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Passe forward à travers le RNN\n",
    "x_in = x_in.permute(1, 0, 2) # Le RNN prend la batch_size en dim 1\n",
    "\n",
    "# On loop sur les pas de temps\n",
    "hiddens = []\n",
    "for t in range(seq_size):\n",
    "    hidden_t = rnn_cell(x_in[t], hidden_t)\n",
    "    hiddens.append(hidden_t)\n",
    "hiddens = torch.stack(hiddens)\n",
    "hiddens = hiddens.permute(1, 0, 2) # on remet la batch_size à la dim 0 (plus logique)\n",
    "print (hiddens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:54:02.857721Z",
     "start_time": "2019-11-28T08:54:02.851135Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aQH4AEW3w3T3"
   },
   "outputs": [],
   "source": [
    "def gather_last_relevant_hidden(hiddens, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(hiddens[batch_index, column_index])\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:54:03.199840Z",
     "start_time": "2019-11-28T08:54:03.194951Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bJTrnijyw3T8",
    "outputId": "b85c10e3-28e5-46ca-8a72-9d6b0ec78889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Gather the last relevant hidden state\n",
    "z = gather_last_relevant_hidden(hiddens, x_lengths)\n",
    "print (z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:54:03.603117Z",
     "start_time": "2019-11-28T08:54:03.594891Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qTcotdAdw3UB",
    "outputId": "10435119-687c-4e3d-b7f5-d1e34b99dbf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "tensor([[0.2885, 0.2099, 0.2444, 0.2572],\n",
      "        [0.2612, 0.2535, 0.2398, 0.2455],\n",
      "        [0.2417, 0.2005, 0.3282, 0.2296],\n",
      "        [0.2126, 0.2512, 0.2470, 0.2892],\n",
      "        [0.1783, 0.3209, 0.2918, 0.2090]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Passe forward dans une couche full-connected \n",
    "fc1 = nn.Linear(rnn_hidden_dim, output_dim)\n",
    "y_pred = fc1(z)\n",
    "y_pred = F.softmax(y_pred, dim=1)\n",
    "print (y_pred.size())\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xmnng9bw3UL"
   },
   "source": [
    "# Manipulation basique de RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SE50HGv3w3UN"
   },
   "source": [
    "Nous pouvons utiliser la couche RNN qui est plus haut-niveau que RNNCell (plus abstraite)\n",
    "pour éviter de faire une boucle (nn.RNN est plus optimisé qu'une boucle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:42:06.436611Z",
     "start_time": "2019-11-28T08:42:06.421375Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2aHwfT_pw3UP",
    "outputId": "0bd7124a-64b9-4765-8963-2776cc582666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:  torch.Size([5, 10, 100])\n",
      "out:  torch.Size([5, 10, 256])\n",
      "h_n:  torch.Size([1, 5, 256])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True) # l'option batch_first=True permet de garder la dim batch en premier\n",
    "out, h_n = rnn(x_in) \n",
    "# out : la série temporelle des prédictions\n",
    "# h_n : le dernier état caché à récupérer pour faire de la classification par exemple\n",
    "\n",
    "print (\"in: \", x_in.size())\n",
    "print (\"out: \", out.size())\n",
    "print (\"h_n: \", h_n.size())\n",
    "\n",
    "# Y a-t'il une différence entre le dernier vecteur de out et h_n ?\n",
    "print(out[0, 9, :10] == h_n[0, 0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T08:53:06.711167Z",
     "start_time": "2019-11-28T08:53:06.700433Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JWzUmB8dw3UU",
    "outputId": "bc3e833f-9738-4cb5-bc89-068968781255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 4])\n",
      "tensor([[[0.2527, 0.1788, 0.1901, 0.2795],\n",
      "         [0.1751, 0.2291, 0.1916, 0.2059],\n",
      "         [0.2314, 0.1812, 0.2669, 0.1534],\n",
      "         [0.1756, 0.2396, 0.1576, 0.1979],\n",
      "         [0.1652, 0.1713, 0.1938, 0.1633]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Passe forward dans une couche full-connected \n",
    "fc1 = nn.Linear(rnn_hidden_dim, output_dim)\n",
    "y_pred = fc1(h_n)\n",
    "y_pred = F.softmax(y_pred, dim=1)\n",
    "print (y_pred.size())\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCjEPyxEw3UY"
   },
   "source": [
    "# Données séquentielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T18:08:39.758749Z",
     "start_time": "2019-11-27T18:08:39.745650Z"
    },
    "colab_type": "text",
    "id": "iAMr9qFqw3Uc"
   },
   "source": [
    "Plusieurs types de tâches séquentielles peuvent être réalisées par des RNNs.\n",
    "\n",
    "1. **One-to-one** : une entrée génère une sortie. \n",
    "    * Ex. Donner un mot et prédire sa catégorie syntaxique (verbe, nom, etc.).\n",
    "    \n",
    "2. **One-to-Many** : une entrée génère plusieurs sorties.\n",
    "    * Ex. Prédire une opinion (positive, négative, etc., on parle de sentiment analysis), générer une critique.\n",
    "\n",
    "3. **Many-to-one** : de nombreuses entrées sont traitées séquentiellement pour générer une seule sortie.\n",
    "    * Ex. Traiter les mots dans une critique pour prédire sa \"valence\" (positive ou négative).\n",
    "\n",
    "4. **Many-to-many** : de nombreuses entrées sont traitées séquentiellement pour générer de nombreuses sorties.\n",
    "    * Ex. Le modèle encode une phrase en français, il traite toute la phrase, puis en produit la traduction anglaise.\n",
    "    * Ex. Étant donnée une série de données chronologiques, prédire la probabilité d'un événement (risque de maladie) à chaque temps.\n",
    "\n",
    "<img src=\"https://github.com/topel/RNN_pytorch_labs/blob/master/tp0_intro_RNN/local/images/seq2seq.jpeg?raw=1\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avmTxHaRw3Uf"
   },
   "source": [
    "# Limites des RNNs \"vanilla\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCWe-hEDw3Un"
   },
   "source": [
    "Il y a plusieurs problèmes avec les RNN simples (dits \"vanilla\" en anglais) que nous avons vus ci-dessus. \n",
    "\n",
    "1. Lorsque nous avons une très longue séquence d'entrée, il devient difficile pour le modèle de conserver l'information vue plus tôt à mesure que nous traitons la séquence. L'objectif du modèle est de conserver les informations utiles des pas de temps précédents, mais cela devient impossible pour une taille de séquence trop grande.\n",
    "\n",
    "2. Pendant la rétropropropagation, le gradient de la fonction de perte doit remonter jusqu'au premier pas de temps. Si notre gradient est supérieur à 1 (${1.01}^{1000} = 20959$) ou inférieur à 1 (${{0.99}^{1000} = 4.31e-5$) et que nous avons beaucoup de pas de temps, cela peut rapidement dégénérer.\n",
    "\n",
    "Pour répondre à ces deux questions, le concept de \"porte\" (\"gate\") a été introduit dans les RNN. Les gates permettent aux RNN de contrôler le flux d'information entre les étapes temporelles afin d'optimiser la tâche à réaliser. Le fait de laisser passer sélectivement l'information permet au modèle de traiter des données séquentielles très longues. Les variantes les plus courantes des RNN sont les unités de mémoire à court terme, appelées [LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM), et les unités récurrentes à \"porte\" [GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU). Vous pouvez en savoir plus sur le fonctionnement de ces unités [ici](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "<img src=\"https://github.com/topel/RNN_pytorch_labs/blob/master/tp0_intro_RNN/local/images/gates.png?raw=1\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:10:39.744612Z",
     "start_time": "2019-11-28T09:10:39.734225Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XQtJEbnUw3Uq"
   },
   "outputs": [],
   "source": [
    "# GRU in PyTorch\n",
    "gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "             batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:10:40.868166Z",
     "start_time": "2019-11-28T09:10:40.862448Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sfSA6MfFw3Ut",
    "outputId": "a301ca70-135f-4ad9-c51e-1058ed57d924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# Initialize synthetic input\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:10:42.170388Z",
     "start_time": "2019-11-28T09:10:42.158020Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "z8jhRIILw3Ux",
    "outputId": "4f8975cb-d294-4d28-953e-0b9f4b2503dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([5, 10, 256])\n",
      "h_n: torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "out, h_n = gru(x_in)\n",
    "print (\"out:\", out.size())\n",
    "print (\"h_n:\", h_n.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKWntYKXw3U2"
   },
   "source": [
    "**Remarque**: Le choix d'utiliser des GRU ou des LSTM dépend des données et des performances empiriques. Les GRU offrent des performances comparables avec un nombre réduit de paramètres, tandis que les LSTM sont plus efficaces et peuvent faire la différence en termes de performances pour une tâche particulière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qIWEa7Vw3U4"
   },
   "source": [
    "# RNNs bidirectionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNLcbGvYw3U5"
   },
   "source": [
    "Beaucoup de progrès ont été réalisés ces dernières années avec les RNN, comme par exemple l'introduction de mécanismes d'[attention](https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow), les Quasi-RNNs, etc. L'une de ces avancées, largement utilisée, sont les RNNNs bidirectionnels (Bi-RNNs). La motivation derrière les RNN bidirectionnels est de traiter une séquence d'entrée dans les deux sens. La prise en compte du contexte dans les deux sens peut améliorer la performance lorsque toute la séquence d'entrée est connue au moment de l'inférence. Une application courante des Bi-RNNs est la traduction automatique : il est avantageux de considérer une phrase entière dans les deux sens pour la traduire dans une autre langue.\n",
    "\n",
    "<img src=\"https://github.com/topel/RNN_pytorch_labs/blob/master/tp0_intro_RNN/local/images/birnn.png?raw=1\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:14:41.013179Z",
     "start_time": "2019-11-28T09:14:40.994274Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lcAohrAuw3U7"
   },
   "outputs": [],
   "source": [
    "# BiGRU en PyTorch\n",
    "bi_gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "                batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:15:35.733207Z",
     "start_time": "2019-11-28T09:15:35.715664Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "POMAdBUWw3VA",
    "outputId": "f18e3ed2-f6fd-49f3-c4c2-02cd5bb98b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([5, 10, 512])\n",
      "h_n: torch.Size([2, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Passe forward \n",
    "out, h_n = bi_gru(x_in)\n",
    "print (\"out:\", out.size()) # tenseur contenant tous les hidden states du RNN\n",
    "print (\"h_n:\", h_n.size()) # le dernier hidden state du RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7n6Yx4xw3Vd"
   },
   "source": [
    "La sortie à chaque temps a une taille de 512, le double de la dim cachée précisée lors de la création de la couche GRU. Cela s'explique par le fait qu'elle inclut à la fois les directions avant et arrière encodées par le BiRNNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1spi_A9Ow3Vs"
   },
   "source": [
    "https://medium.com/dair-ai/building-rnns-is-fun-with-pytorch-and-google-colab-3903ea9a3a79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xffhivSExSBL"
   },
   "source": [
    "# Gérer les séquences de taille variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:32:34.843284Z",
     "start_time": "2020-03-09T10:32:34.839142Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ii-VS_s9w3V5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDM6GTlx4MTP"
   },
   "source": [
    "## Solution 1 : zero-padding\n",
    "\n",
    "Pour construire un tensor qui regroupe des séquences de taille différente, on complète les séquences avec des zéros jusqu'à obtenir la taille de la séquence la plus grande.\n",
    "\n",
    "\n",
    "Cela est réalisé à l'aide de pad_sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:19:49.552122Z",
     "start_time": "2020-03-09T10:19:49.527813Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kE3Di-AlxWLO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 6.],\n",
       "        [2., 5., 0.],\n",
       "        [3., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sur des séquences 1-d\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5])\n",
    "c = torch.Tensor([6])\n",
    "rnn_utils.pad_sequence([a, b, c], batch_first=False, padding_value=0) # ce sont les arguments par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:19:52.537716Z",
     "start_time": "2020-03-09T10:19:52.528769Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7wziU0B90fq3",
    "outputId": "86678d8e-9936-43b6-d661-45e7f60a5eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 100])\n"
     ]
    }
   ],
   "source": [
    "# sur des séquences 2-d (ici des \"embeddings\")\n",
    "embedding_dim = 100\n",
    "lengths = [10, 15, 20]\n",
    "a = torch.randn(lengths[0], embedding_dim)\n",
    "b = torch.randn(lengths[1], embedding_dim)\n",
    "c = torch.randn(lengths[2], embedding_dim)\n",
    "\n",
    "x_in = rnn_utils.pad_sequence([a, b, c])\n",
    "print (x_in.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbzHstcT8LQm"
   },
   "source": [
    "On a obtenu un tenseur de dimensions : T x B x d\n",
    "\n",
    "La fonction pad_sequence suppose que les dimensions des séquences, autres que leur taille, sont identiques. D'autre part, les tenseurs doivent être de même type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "694DWfiv8tTM"
   },
   "source": [
    "## Solution 2 : \"empaqueter\" les séquences avec pack_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:21:18.604216Z",
     "start_time": "2020-03-09T10:21:18.594570Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lLq4lKwV0x0F",
    "outputId": "1bc5b32d-caff-4ecb-94b3-e6cbaeb64ba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1., 4., 6., 2., 5., 3.]), batch_sizes=tensor([3, 2, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sur des séquences 1-d\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5])\n",
    "c = torch.Tensor([6])\n",
    "# rnn_utils.pack_sequence([a, b, c], enforce_sorted=True) # enforce_sorted: vérifie que les sequences sont déjà ordenées par longueur décroissante\n",
    "rnn_utils.pack_sequence([a, b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:22:51.359200Z",
     "start_time": "2020-03-09T10:22:51.347235Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "H1uw-tVP87kZ",
    "outputId": "9995087c-b422-4ff9-8d6e-04002b370c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 100])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1])\n",
      "20\n",
      "tensor(45)\n"
     ]
    }
   ],
   "source": [
    "# sur des séquences 2-d (ici des \"embeddings\")\n",
    "embedding_dim = 100\n",
    "lengths = [10, 15, 20]\n",
    "# lengths = lengths[::-1]\n",
    "a = torch.randn(lengths[0], embedding_dim)\n",
    "b = torch.randn(lengths[1], embedding_dim)\n",
    "c = torch.randn(lengths[2], embedding_dim)\n",
    "\n",
    "# x_in = rnn_utils.pack_sequence([a, b, c], enforce_sorted=False)\n",
    "# x_in = rnn_utils.pack_sequence([a, b, c])\n",
    "# --> RuntimeError: 'lengths' array has to be sorted in decreasing order\n",
    "x_in = rnn_utils.pack_sequence([c, b, a])\n",
    "\n",
    "print (x_in.data.size())\n",
    "print(x_in.batch_sizes)\n",
    "print(len(x_in.batch_sizes))\n",
    "print(x_in.batch_sizes.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sA8QMgFxB7UP"
   },
   "source": [
    "## Solution 3 : pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:23:27.672766Z",
     "start_time": "2020-03-09T10:23:27.661395Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bYM0rpKLB6Hf",
    "outputId": "448492da-3a72-490f-8ff6-ccd8c7dd1301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 100])\n",
      "torch.Size([45, 100]) tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# sur des séquences 2-d (ici des \"embeddings\")\n",
    "embedding_dim = 100\n",
    "lengths = [10, 15, 20]\n",
    "lengths = lengths[::-1]\n",
    "a = torch.randn(lengths[0], embedding_dim)\n",
    "b = torch.randn(lengths[1], embedding_dim)\n",
    "c = torch.randn(lengths[2], embedding_dim)\n",
    "\n",
    "x_pad = rnn_utils.pad_sequence([a, b, c])\n",
    "print (x_pad.size())\n",
    "\n",
    "# x_in = rnn_utils.pack_padded_sequence(x_pad, torch.Tensor(lengths),  enforce_sorted=False)\n",
    "x_in = rnn_utils.pack_padded_sequence(x_pad, torch.Tensor(lengths))\n",
    "print(x_in.data.size(), x_in.batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAXOtK8o-fLW"
   },
   "source": [
    "## Application : mini-batch de sequences de tailles variables dans un RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:17:20.290233Z",
     "start_time": "2020-03-09T12:17:20.277972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 100])\n",
      "torch.Size([45, 100]) tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# sur des séquences 2-d (ici des \"embeddings\")\n",
    "embedding_dim = 100\n",
    "lengths = [10, 15, 20]\n",
    "lengths = lengths[::-1]\n",
    "a = torch.randn(lengths[0], embedding_dim)\n",
    "b = torch.randn(lengths[1], embedding_dim)\n",
    "c = torch.randn(lengths[2], embedding_dim)\n",
    "\n",
    "x_pad = rnn_utils.pad_sequence([a, b, c], batch_first=False)\n",
    "print (x_pad.size())\n",
    "\n",
    "# x_in = rnn_utils.pack_padded_sequence(x_pad, torch.Tensor(lengths),  enforce_sorted=False)\n",
    "x_in = rnn_utils.pack_padded_sequence(x_pad, torch.Tensor(lengths), batch_first=False)\n",
    "print(x_in.data.size(), x_in.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:39:53.849202Z",
     "start_time": "2020-03-09T12:39:53.833991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 10]) torch.Size([1, 3, 10])\n",
      "<class 'torch.nn.utils.rnn.PackedSequence'>\n"
     ]
    }
   ],
   "source": [
    "# now run through LSTM\n",
    "num_layers=1\n",
    "hidden_size=10\n",
    "batch_size=3\n",
    "\n",
    "rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "        )\n",
    "\n",
    "h0 = Variable(torch.randn(num_layers, batch_size, hidden_size))\n",
    "c0 = Variable(torch.randn(num_layers, batch_size, hidden_size))\n",
    "\n",
    "out, (h, c) = rnn(x_in, (h0, c0))\n",
    "# \n",
    "# h: hidden state for t = seq_len \n",
    "# c: cell state for t = seq_len\n",
    "print(h.size(), c.size()) \n",
    "print(type(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:27:16.843142Z",
     "start_time": "2020-03-09T12:27:16.826763Z"
    }
   },
   "source": [
    "out est un objet PackedSequence. On peut défaire le packing avec pad_packed_sequence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:39:26.905581Z",
     "start_time": "2020-03-09T12:39:26.897973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# undo the packing operation\n",
    "out_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=False)\n",
    "out_unpacked.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:42:09.268726Z",
     "start_time": "2020-03-09T12:42:09.261306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 10])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour passer out à une couche linéaire, on fait un reshape, avec un contiguous avant pour\n",
    "# accélérer les accès mémoire : \n",
    "out_vec = out_unpacked.contiguous()\n",
    "out_vec = out_vec.view(-1, out_vec.shape[2])\n",
    "out_vec.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out_vec est maintenant une matrice de size (T*B, H).\n",
    "\n",
    "On peut appliquer maintenant une couche linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:53:37.625961Z",
     "start_time": "2020-03-09T12:53:37.618433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 5\n",
    "classif_layer = nn.Linear(hidden_size, num_classes)\n",
    "classif_out = classif_layer(out_vec)\n",
    "classif_out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ffaire un reshape pour récupérer des séquences de prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T12:54:05.058849Z",
     "start_time": "2020-03-09T12:54:05.053563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "preds = classif_out.reshape(-1, batch_size, num_classes)\n",
    "print(preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TP1_intro_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
