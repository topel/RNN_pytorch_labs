{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Sujet-de-TP-:-modèle-de-langage-avec-un-RNN-&quot;Vanilla&quot;\" data-toc-modified-id=\"Sujet-de-TP-:-modèle-de-langage-avec-un-RNN-&quot;Vanilla&quot;-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sujet de TP : modèle de langage avec un RNN \"Vanilla\"</a></span></li><li><span><a href=\"#Préparation-des-données\" data-toc-modified-id=\"Préparation-des-données-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Préparation des données</a></span></li><li><span><a href=\"#imports-et-accès-à-un-GPU\" data-toc-modified-id=\"imports-et-accès-à-un-GPU-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>imports et accès à un GPU</a></span></li><li><span><a href=\"#Chargement-du-corpus\" data-toc-modified-id=\"Chargement-du-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Chargement du corpus</a></span></li><li><span><a href=\"#Créer-la-classe-du-modèle\" data-toc-modified-id=\"Créer-la-classe-du-modèle-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Créer la classe du modèle</a></span></li><li><span><a href=\"#Instancier-le-modèle\" data-toc-modified-id=\"Instancier-le-modèle-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Instancier le modèle</a></span></li><li><span><a href=\"#Apprentissage\" data-toc-modified-id=\"Apprentissage-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Apprentissage</a></span></li><li><span><a href=\"#Tester-le-modèle-sur-des-phrases\" data-toc-modified-id=\"Tester-le-modèle-sur-des-phrases-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Tester le modèle sur des phrases</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sujet de TP : modèle de langage avec un RNN \"Vanilla\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous allons créer un RNN simple (un \"Vanilla RNN\") qui prédit le mot qui suit un début de phrase.\n",
    "\n",
    "Nous allons entraîner ce RNN sur un tout petit sous-ensemble de textes provenant du corpus Librivox French (https://librivox.org) qui regroupe des audiobooks.\n",
    "\n",
    "Une fois entraîné, vous pouvez compléter un début de phrase en faisant des prédictions avec le modèle.\n",
    "\n",
    "Nous utilisons les balises < unk > et < eos > pour les mots qui ne sont pas dans notre vocabulaire et pour indiquer une fin de phrase, respectivement.\n",
    "    \n",
    "Nous avons restreint le vocabulaire à la liste de mots apparaissant au moins 4 fois dans Librivox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous fournissons un notebook tout prêt pour cela : generate_librivox_fr.ipynb\n",
    "\n",
    "Ouvrez-le et lisez-le. Tentez de comprendre à quoi sert chaque objet et chaque cellule du notebook.\n",
    "\n",
    "Exécutez chaque cellule. \n",
    "\n",
    "Quels fichiers ont été créés ? Que contiennent-ils ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports et accès à un GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T12:04:43.782274Z",
     "start_time": "2019-11-20T12:04:43.770070Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pour Google Colab\n",
    "# import sys, os\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/gdrive')\n",
    "#     file_name = 'vrnn_demo.ipynb'\n",
    "#     import subprocess\n",
    "#     path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "#     print(path_to_file)\n",
    "#     path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "#     os.chdir(path_to_file)\n",
    "#     !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:28:00.786254Z",
     "start_time": "2019-11-25T12:28:00.781212Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ou sans GPU ?\n",
    "\n",
    "Il est recommandé d'exécuter ce code sur GPU :<br> \n",
    "* Temps pour 1 epoch sur CPU : 153 sec ( 2.55 min)<br> \n",
    "* Temps pour 1 epoch sur GPU : 8.4 sec avec une GeForce GTX 1080 Ti <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:52:57.175685Z",
     "start_time": "2019-11-25T08:52:57.171141Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:52:57.951986Z",
     "start_time": "2019-11-25T08:52:57.417153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device, torch.cuda.get_device_name(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:52:57.983918Z",
     "start_time": "2019-11-25T08:52:57.955336Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que vous avez bien généré les données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:55:41.217055Z",
     "start_time": "2019-11-25T08:55:41.208720Z"
    }
   },
   "outputs": [],
   "source": [
    "path_data='../'\n",
    "\n",
    "flag_train_data = os.path.isfile(path_data + 'librivox_fr/train_data.pt') \n",
    "flag_test_data = os.path.isfile(path_data + 'librivox_fr/test_data.pt') \n",
    "\n",
    "flag_idx2word = os.path.isfile(path_data + 'librivox_fr/idx2word.pt') \n",
    "flag_word2idx = os.path.isfile(path_data + 'librivox_fr/word2idx.pt') \n",
    "\n",
    "if flag_idx2word==False or flag_test_data==False or flag_train_data==False or flag_word2idx==False:\n",
    "    print('Librivox_fr dataset manquant')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger train_data et test_data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:56:24.016595Z",
     "start_time": "2019-11-25T08:56:24.002089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20542, 20])\n",
      "torch.Size([74, 20])\n"
     ]
    }
   ],
   "source": [
    "train_data  =  torch.load(path_data+'librivox_fr/train_data.pt')\n",
    "test_data   =  torch.load(path_data+'librivox_fr/test_data.pt')\n",
    "\n",
    "print(train_data.size())\n",
    "print(test_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les dictionnaires idx2word et word2idx :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T09:01:16.555081Z",
     "start_time": "2019-11-25T09:01:16.539192Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx  =  torch.load(path_data + 'librivox_fr/word2idx.pt')\n",
    "idx2word  =  torch.load(path_data + 'librivox_fr/idx2word.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la première phrase du texte train_librivox_fr_50words_max_15200.txt est : \n",
    "\n",
    "\"enfant si j’étais roi je donnerais l’empire et mon char et mon sceptre et mon peuple à genoux et ma couronne d’or et mes bains de porphyre et mes flottes à qui la mer ne peut suffire\"\n",
    "\n",
    "Comment est-elle stockée dans le tenseur train_data ?\n",
    "\n",
    "Afficher les 38 premiers mots dans train_data qui correspondent à cette phrase jusqu'à < eos >. Où est stockée la deuxième phrase ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T10:17:01.885684Z",
     "start_time": "2019-11-25T10:17:01.873377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase index= 0\n",
      "1:enfant 2:si 3:j’étais 4:roi 5:je 6:donnerais 7:l’empire 8:et 9:mon 10:char 8:et 9:mon 11:sceptre 8:et 9:mon 12:peuple 13:à 14:genoux 8:et 15:ma 16:couronne 17:d’or 8:et 18:mes 19:bains 20:de 21:porphyre 8:et 18:mes 0:<unk> 13:à 22:qui 23:la 24:mer 25:ne 26:peut 27:suffire 28:<eos> "
     ]
    }
   ],
   "source": [
    "seq_length = 38\n",
    "\n",
    "phrase_index = 0\n",
    "print(\"phrase index=\", phrase_index)\n",
    "\n",
    "# create a minibatch\n",
    "minibatch_data =  train_data[:seq_length]\n",
    "#     print(minibatch_data)\n",
    "for i in range(seq_length):\n",
    "    print(\"%d:%s\"%(minibatch_data[i][phrase_index], idx2word[minibatch_data[i][phrase_index]]), end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En déduire comment les données sont présentées au futur modèle.\n",
    "\n",
    "Rep : en ligne les phrases et en colonne les batch. C'est-à-dire que les mots des premières lignes du ligne sont stockées dans la première colonne jusqu'à atteindre 20542 mots, puis on passe à la deuxième colonne, etc. Il y aura la troisième dimension qui sera la taille des embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T09:07:49.967419Z",
     "start_time": "2019-11-25T09:07:29.838Z"
    }
   },
   "outputs": [],
   "source": [
    "20*20542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:41:30.174110Z",
     "start_time": "2019-11-25T08:41:30.067802Z"
    }
   },
   "outputs": [],
   "source": [
    "max(torch.unique(train_data)), len((torch.unique(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T08:41:30.816681Z",
     "start_time": "2019-11-25T08:41:30.803677Z"
    }
   },
   "outputs": [],
   "source": [
    "max(torch.unique(test_data)), len((torch.unique(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:32:12.463892Z",
     "start_time": "2019-11-20T16:32:12.458417Z"
    }
   },
   "source": [
    "l'indice 9575 dépasse le tableau ---> bug dans test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques constantes associées au dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T09:34:04.814064Z",
     "start_time": "2019-11-25T09:34:04.809421Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 20 # taille des batches : si modifiée, il faut regénérer train et test avec ce bs dans generate_librivox_fr\n",
    "seq_length = 35 # taille des \"phrases\" à donner au réseau\n",
    "\n",
    "# vocab_size = 17498 # if WORD_OCC_THRESHOLD == 1\n",
    "vocab_size = 9574 # if WORD_OCC_THRESHOLD == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer la classe du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la définition du modèle à trois couches suivant, en indiquant les bonnes dimensions.\n",
    "\n",
    "Compléter la définition de la fonction forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:12:10.413336Z",
     "start_time": "2019-11-25T12:12:10.402225Z"
    }
   },
   "outputs": [],
   "source": [
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        \n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.RNN(       hidden_size , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instancier le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrire une fonction qui affiche le nombre de paramètres d'un réseau donné en argument (net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:39.332301Z",
     "start_time": "2019-11-25T12:17:39.326001Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Nombre de paramètres du réseau : {} ({:.2f} millions)'.format(\n",
    "        nb_param, nb_param/1e6)\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choisir une hidden size de 150. Combien de paramètres contient le modèle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:45.526600Z",
     "start_time": "2019-11-25T12:17:45.459895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_recurrent_net(\n",
      "  (layer1): Embedding(9574, 150)\n",
      "  (layer2): RNN(150, 150)\n",
      "  (layer3): Linear(in_features=150, out_features=9574, bias=True)\n",
      ")\n",
      "Nombre de paramètres du réseau : 2927074 (2.93 millions)\n"
     ]
    }
   ],
   "source": [
    "hidden_size=150\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envoyer le modèle sur le GPU (si vous utilisez un gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:52.954047Z",
     "start_time": "2019-11-25T12:17:49.621701Z"
    }
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialiser les poids de la couche embedding et de la couche linéaire avec une distribution uniforme sur [-0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:53.816642Z",
     "start_time": "2019-11-25T12:17:53.805667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir la fonction de coût entropie croisée et les hyperparamètres suivants : \n",
    "* learning rate initial : my_lr=1\n",
    "* taille des séquences : seq_length=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:56.841633Z",
     "start_time": "2019-11-25T12:17:56.837059Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 1.\n",
    "\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'apprentissage, pour éviter le phénomène d'explosion du gradient, nous allons utiliser une fonction qui normalise les valeurs du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:59.533762Z",
     "start_time": "2019-11-25T12:17:59.525152Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "   \n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('norme du gradient proche de zéro')\n",
    "    else:\n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une fonction qui évalue le réseau sur le jeu de test (non-utilisée car bug dans données pour l'instant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:17:57.792933Z",
     "start_time": "2019-11-25T12:17:57.781421Z"
    }
   },
   "outputs": [],
   "source": [
    "# def eval_on_test_set():\n",
    "\n",
    "#     running_loss=0\n",
    "#     num_batches=0    \n",
    "       \n",
    "#     h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "#     h=h.to(device)\n",
    "\n",
    "#     for count in range( 0 , 74-seq_length ,  seq_length) :\n",
    "               \n",
    "#         minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "#         minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "#         minibatch_data=minibatch_data.to(device)\n",
    "#         minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "#         scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "#         minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "#         scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "#         loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "#         h=h.detach()\n",
    "            \n",
    "#         running_loss += loss.item()\n",
    "#         num_batches += 1        \n",
    "    \n",
    "#     total_loss = running_loss/num_batches \n",
    "#     print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la boucle d'apprentissage aux endroits indiqués et entraîner le modèle sur 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:18:18.636731Z",
     "start_time": "2019-11-25T12:18:15.350442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([35, 20])\n",
      "Y: torch.Size([35, 20])\n",
      "scores: torch.Size([35, 20, 9574])\n",
      "h: torch.Size([1, 20, 150])\n",
      "\n",
      "epoch= 0 \t time= 3.2571892738342285 \t lr= 1.0 \t exp(loss)= 717.5571866956979\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    # garder le learning rate à 1 pour les 4 premières epochs, puis diviser par 1.1 à chaque epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    \n",
    "    # créer un nouvel optimizer et lui passer le learning rate actualisé.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # initialisation du coût et du nombre de batchs à chaque nouvelle epoch \n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    \n",
    "    # initialiser h par un vecteur de zéros :\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # envoi au gpu    \n",
    "    h=h.to(device)\n",
    "    \n",
    "    for count in range( 0 , 20542-seq_length ,  seq_length):\n",
    "             \n",
    "        # Mettre les gradients à zéro\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # créer un minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        if count == 0:\n",
    "            print('X:', minibatch_data.shape)\n",
    "            print('Y:', minibatch_label.shape)\n",
    "        \n",
    "        # envoi au gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detacher h pour ne pas backpropager sur toutes les séquences depuis le début de l'epoch\n",
    "        h=h.detach()\n",
    "        # Dire à Pytorch de tracker les opérations sur h pour le minibatch courant\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # Passe forward\n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        if count == 0:\n",
    "            print('scores:', scores.shape)\n",
    "            print('h:', h.shape)\n",
    "        \n",
    "        # Reshape les tenseurs scores et labels pour obtenir une longueur de bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Calculer la loss moyenne\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # Passe backward pour calculer les gradients dL/dR, dL/dV et dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # Normaliser les gradients et faire une itération de SGD : R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Actualiser le coût par epoch et le nb de batches traités  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # calcul du coût sur tout le training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    \n",
    "    # Estimer la performance sur le jeu de test (bug pour l'instant) \n",
    "    #     eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : quelle est la taille des tenseurs suivants ?\n",
    "        - minibatch_data, minibatch_label,\n",
    "        - h, scores avant le reshape\n",
    "        - h, scores après le reshape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tester le modèle sur des phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une fonction qui prend une phrase et qui la convertit en tenseur exploitable pour le réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:27:17.276838Z",
     "start_time": "2019-11-25T12:27:17.267440Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence2vector_librivox_fr(sentence):\n",
    "    words = sentence.split()\n",
    "    x = torch.LongTensor(len(words),1)\n",
    "    for idx, word in enumerate(words):\n",
    "        word = re.sub(\"'\", \"_\", word)\n",
    "        if word not in word2idx:\n",
    "            print('Vous avez entrer un mot hors-vocabulaire.')\n",
    "            print('--> Enlever lettres majuscules et ponctuation')\n",
    "            print(\"mot --> <unk> avec index 0\")\n",
    "            x[idx,0]=0            \n",
    "        else:\n",
    "            x[idx,0]=word2idx[word]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:27:20.654581Z",
     "start_time": "2019-11-25T12:27:20.649399Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence1 = \"on entendait vaguement au dehors les\"\n",
    "\n",
    "sentence2 = \"hier je luttai de la sorte contre le grand\"\n",
    "\n",
    "sentence3 = \"il connaissait la route et nous avons\"\n",
    "\n",
    "# ou bien créer votre propre phrase. Il ne fauit pas utiliser de majuscules ni de ponctuation.\n",
    "# Chaque mot doit être dans le lexique.\n",
    "sentence4= \"il est beaucoup\"\n",
    "\n",
    "# Choisir le phrase ici : \n",
    "mysentence = sentence1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir la phrase choisie et envoi au GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:29:05.196965Z",
     "start_time": "2019-11-25T12:29:05.173523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 366],\n",
      "        [1383],\n",
      "        [1884],\n",
      "        [  72],\n",
      "        [ 127],\n",
      "        [  37]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "minibatch_data=sentence2vector_librivox_fr(mysentence)\n",
    "      \n",
    "minibatch_data=minibatch_data.to(device)\n",
    "\n",
    "print(minibatch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir un hidden state initial à zero, et exécuter le RNN sur la phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:30:13.138008Z",
     "start_time": "2019-11-25T12:30:13.117901Z"
    }
   },
   "outputs": [],
   "source": [
    "h = torch.zeros(1, 1, hidden_size)\n",
    "h=h.to(device)\n",
    "\n",
    "scores , h = net( minibatch_data , h )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrire une fonction show_next_word qui prend en entrée scores et qui affiche les 30 mots les plus probables prédits par le réseau.\n",
    "\n",
    "Vous utiliserez la fonction torch.topk()\n",
    "Aide : https://pytorch.org/docs/stable/torch.html?highlight=topk#torch.topk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:43:37.828876Z",
     "start_time": "2019-11-25T12:43:37.820802Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_next_word(scores):\n",
    "    num_word_display=30\n",
    "    prob = F.softmax(scores,dim=2)\n",
    "    p = prob[-1].squeeze()\n",
    "    p, word_idx = torch.topk(p, num_word_display)\n",
    "\n",
    "    for i,idx in enumerate(word_idx):\n",
    "        percentage= p[i].item()*100\n",
    "        word=  idx2word[idx.item()]\n",
    "        print(  \"{:.1f}%\\t\".format(percentage),  word ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la prédiction du prochain mot par le réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:43:43.410585Z",
     "start_time": "2019-11-25T12:43:43.375888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on entendait vaguement au dehors les ... \n",
      "\n",
      "5.5%\t <unk>\n",
      "1.3%\t grand\n",
      "0.9%\t jour\n",
      "0.8%\t plus\n",
      "0.7%\t deux\n",
      "0.7%\t yeux\n",
      "0.7%\t calife\n",
      "0.5%\t sultan\n",
      "0.5%\t voix\n",
      "0.4%\t même\n",
      "0.4%\t main\n",
      "0.4%\t soir\n",
      "0.4%\t peu\n",
      "0.4%\t nuit\n",
      "0.4%\t autres\n",
      "0.4%\t vie\n",
      "0.3%\t hommes\n",
      "0.3%\t homme\n",
      "0.3%\t tête\n",
      "0.3%\t maison\n",
      "0.3%\t père\n",
      "0.3%\t temps\n",
      "0.3%\t place\n",
      "0.3%\t monde\n",
      "0.3%\t ville\n",
      "0.3%\t génie\n",
      "0.3%\t petite\n",
      "0.3%\t loup\n",
      "0.3%\t bras\n",
      "0.3%\t roi\n"
     ]
    }
   ],
   "source": [
    "print(mysentence, '... \\n')\n",
    "\n",
    "show_next_word(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mot < unk > sera presque toujours le plus probable. Écrire une fonction get_next_word, variante de show_next_word qui retourne le mot le plus fréquent. Si ce mot est < unk >, votre fonction doit retourner le duexième mot le plus probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:47:19.238737Z",
     "start_time": "2019-11-25T12:47:19.231031Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_word(scores):\n",
    "    prob=F.softmax(scores,dim=2)\n",
    "    num_word_display = 2\n",
    "    p=prob[-1].squeeze()\n",
    "    p, word_idx = torch.topk(p, num_word_display)\n",
    "#     print(p, word_idx)\n",
    "    if word_idx[0] == 0:\n",
    "        return idx2word[word_idx[1]]\n",
    "    else:\n",
    "        return idx2word[word_idx[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:47:22.117171Z",
     "start_time": "2019-11-25T12:47:22.096049Z"
    }
   },
   "outputs": [],
   "source": [
    "next_word = get_next_word(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Écrire un bout de code qui prédit une phrase entière à partir de mysentence.\n",
    "\n",
    "Cette phrase sera considérée comme terminée sur le modèle prédit < eos > ou bien après 10 itérations maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:50:00.268876Z",
     "start_time": "2019-11-25T12:50:00.241755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on entendait vaguement au dehors les grand <eos>...\n",
      "on entendait vaguement au dehors les grand <eos> et...\n",
      "on entendait vaguement au dehors les grand <eos> et <eos>...\n"
     ]
    }
   ],
   "source": [
    "print(mysentence + '...')\n",
    "\n",
    "i= 0\n",
    "not_finished = True\n",
    "while i < 10 and not_finished :\n",
    "\n",
    "    minibatch_data = sentence2vector_librivox_fr(mysentence)\n",
    "      \n",
    "    minibatch_data = minibatch_data.to(device)\n",
    "    h = torch.zeros(1, 1, hidden_size)\n",
    "    h=h.to(device)\n",
    "    scores , h = net( minibatch_data , h )\n",
    "    \n",
    "    next_word = get_next_word(scores)\n",
    "    mysentence += ' ' + next_word\n",
    "    print(mysentence + '...')\n",
    "    not_finished = next_word != '<eos>'\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
